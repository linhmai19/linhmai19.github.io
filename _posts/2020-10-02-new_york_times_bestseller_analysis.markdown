---
layout: post
title:      "NEW YORK TIMES BESTSELLER ANALYSIS"
date:       2020-10-02 22:09:52 +0000
permalink:  new_york_times_bestseller_analysis
---


Having the line “New York Times Bestsellers” on their book cover is the dream of most people who are in the writing field. Since 1931, the New York Times Bestseller list has become one of the most powerful tools for authors to validate their popularity, for publishers to increase their exposure and as a guideline, for readers to choose what books to read in their precious spare time. Therefore, becoming a New York Times best-seller has a great impact on a book’s sales, especially for books by debut authors. On average, it helps increase the sale by about 14% and for a new author, it can help increase the sale by more than 50%. Besides the sale increase, this prestigious list provides the award-like title to the authors, just like a movie director who receives the Academy Award. However, according to the data gathered by data.world from New York Times Bestseller, since 2016, the bestseller trend has decreased. What does it take to be named a best-seller? That is for sure the question many authors and publishers want to know and this analysis will help them have an initial understanding of the causes and the factors that affect the result of New York Times Bestseller.

In this project, data was gathered from two different sources: existing dataset from the website data.world and data scratched from the website Goodreads.

### 1. The raw dataset on data.world about New York Times Bestsellers from 2011 to 2018 contains 2248 data points and 14 features

* There are three columns ‘book_review_link’, ‘sunday_review_link’, and ‘first_chapter_link’ that contain 94% to 99% missing values which are useless for analysis, so they were dropped from the dataset. Even though the columns ‘age_group’, ‘dagger’, and ‘price’ seem to be useful, all of these columns contain all null values or ‘0’ value which indicates that there is no information on the age group, dagger, or price for the books. As a result, these three columns were also dropped from the dataset as there is no use to keep them for building model later.
 * ISBN is a unique ID for each book. Therefore, we cannot replace or fill it with the mean or median of all other values in the same columns. The missing values which indicate as ‘NA’ in ‘primary_isbn10’ are about 7% of the entire data, but there are also rows that contain the string ‘None’. Together, the missing values and ‘None’ values are about 35% of the entire dataset. Therefore, we cannot drop them from the dataset. Instead, null values were replaced with the string ‘None’ and they were treated as its own category. On the other hand, a lot of ISBN values have only 8 or 9 digits. It is because the first digit(s) is supposed to be 0(‘s). Therefore, ‘00’ was added as a prefix for 8-digit ISBNs and ‘0’ as a prefix for 9-digit ISBNs.
 * Two new columns were added to this dataset. First of all, the column feature, called ‘no_of_words_title’ was also generated by counting the number of words in each of the book titles. Secondly, the target column, called ‘popularity’ was generated to turn the project into a classification modeling type. Based on the data from the column ‘weeks_on_list’, the new target column gives the output ‘True’ for books that stay on New York Times Bestseller list for more than 5 weeks and ‘False’ for books that stay on the list less than 5 weeks.


### 2. Data about book review statistics, given the ISBNs list from data.world through fetching Goodreads API or Application Programming Interface
First of all the ISBNs were extracted as a list from the dataset of data.world. It was then used to draw the book review statistics from the Goodreads API. A lot of these ISBNs were eliminated because they are not real or existing, so book review statistics cannot be pulled from Goodreads. After fetching, there are 1287 data points left and these data were saved as nested list-dictionary in a JSON file. It was then converted into a DataFrame for easier cleaning and saving as a CSV file.
The two dataframes were merged and further cleaning was performed to eliminated outliers, as well as duplicates. The categorical features ‘publisher’ and ‘author’ in this merged dataset are not ordinal. Besides, the numbers of categories in these features are quite large. Therefore, we cannot use Label Encoding and One Hot Encoding as these two methods require that the categorical feature is ordinal (Label Encoding) and the number of categories is small (One Hot Encoding). These two categorical features were encoded by creating dummy variables. With continuous feature columns, these variables were normalized by using MinMaxScaler(). This process of rescaling one or more attributes to the range of 0 to 1 to bring the features to a similar scale. This means that the largest value for each attribute is 1 and the smallest value is 0.

```
cat_feats = [‘publisher’, ‘author’]
# Get dummies variables for ‘publisher’ and ‘author’ in the merged dataframe
df_merge = pd.concat([df_merge,  
                      pd.get_dummies(df_merge[‘publisher’], 
                      prefix=’dummy’, drop_first=True)], axis=1)
df_merge = pd.concat([df_merge, pd.get_dummies(df_merge[‘author’], 
                      prefix=’dummy’, drop_first=True)], axis=1)
df_merge.drop([‘publisher’, ‘author’], axis=1, inplace=True)
con_feats = ['no_of_words_title','ratings_count',
             'reviews_count','text_reviews_count',
             'work_ratings_count','work_reviews_count',
             'work_text_reviews_count','average_rating']
# Define min max scaler
scaler = MinMaxScaler()
# Transform data
scaled = scaler.fit_transform(df_merge[con_feats])
scaled = pd.DataFrame(scaled, columns=con_feats)
# Drop the original values and merge the scaled values for continuous columns
df1 = df_merge.drop(columns=con_feats, axis = 1)
df1 = df1.merge(scaled, left_index=True, 
                right_index=True, how = "left")
```

The classification modeling process was done on two datasets: the merged dataset with fewer data points and more features, and the original dataset with more data points but fewer features. In this way, we were able to compare how well the two datasets’ models perform and whether it was worth the tradeoff of data points with additional features. 
The baseline models for two datasets were built with Ordinary Least Squares (OLS). The result R-squared of the merged dataset model (0.572) is slightly higher than that of the original dataset (0.536) with more data points. Initially, we can see that although there is a slight improvement in model performance, adding more features seem to be a great way to go. 
For this project, I used five different classifiers to build my models: Random Forest, K-Nearest Neighbor, Decision Tree, XGBoost, and MLP (Neural Network). Each of the models was run through a hyperparameter grid search by GridSearchCV with defined parameter grids and StratifiedKFold of 5 splits.

```
def gridsearch_cv(clf, params, X_train, y_train, cv):
    pipeline = Pipeline([('clf', clf)])
    gs = GridSearchCV(pipeline, params, cv=kf, n_jobs=-1,
                      scoring='f1', return_train_score=True)
    gs.fit(X_train, y_train)
    return gs
random_forest_params = {'clf__max_depth': [25, 50, 75],
                        'clf__max_features': ['sqrt'],
                        'clf__criterion': ['gini', 'entropy'],
                        'clf__n_estimators': [100, 300, 500, 1000]}
knn_params = {'clf__n_neighbors': [5, 15, 25, 35, 45, 55, 65],
              'clf__weights': ['uniform', 'distance'],
              'clf__p': [1, 2, 10]}
decision_tree_params = {'clf__max_depth': [25, 50, 75],
                        'clf__max_features': ['sqrt'],
                        'clf__criterion': ['gini', 'entropy'],
                        'clf__min_samples_split': [6, 10, 14],}
xgb_params = {'clf__learning_rate': [0.1],
              'clf__max_depth': [1, 3, 6],
              'clf__n_estimators' : [30, 100, 250],
              'clf__min_child_weight': [4, 6, 10],
              'clf__subsample': [0.7]}
mlp_params = {'clf__hidden_layer_sizes': 
                                [(50,50,50), (50,100,50), (100,)],
              'clf__activation': ['tanh', 'relu'],
              'clf__solver': ['sgd', 'adam'],
              'clf__alpha': [0.0001, 0.05],
              'clf__learning_rate': ['constant','adaptive']}
```

### Overall Metrics Results

* Accuracy: the ratio of all correctly predicted cases whether positive or negative and all cases in the data. (How often is the classifier correct?)
Can be misleading with unbalanced datasets
* Precision: the ratio of correctly predicted positive cases vs. all predicted positive cases. (When it predicts yes, how often is it correct?)
* Recall: the ratio of all correctly identified positive cases divided by all actually positive cases. (When it is actually yes, how often does it predict yes?)
Can get a 100% recall score by simply assuming every case is true, which would not necessarily be a better model
* F1: The weighted average of precision and recall. It takes both false positives and false negatives into account

To sum, the models of the merged dataset without fewer data points and more features generally perform better than the original dataset. Therefore, trading off data points with additional features would help us understand more about the main factors that affect the outcome of the New York Times Bestseller.

### The Most Important Features

K-Nearest Neighbor, Random Forest, and MLP are the top 3 models that perform the best, based on precision and AUC scores. The feature importance was obtained to identify which factors affect a book to be on the New York Times Bestseller list for more than 5 weeks or not.

* work_ratings_count: the total number of numerical ratings of all editions
* work_reviews_count: the total number of reviews of all editions
* work_text_reviews_count: the total number of text-only reviews of all editions
* reviews_count: the total number of reviews a book received
* ratings_count: the total number of numerical ratings a book has

There is a very important point that should also be noticed in this analysis. The most important feature that was generated from the model of the original dataset is the number of words in a book title. By combining with the top important features from the models of the merged dataset, it shows that the top 20 books with the most review count all have 6 words in the title. Therefore, this feature should also be taken into consideration as one of the important factors that affect the outcome of the New York Times Bestseller.
